IA Street Fighter model reinforcement learning
----------------------------------------------

Lien de la video youtube: https://www.youtube.com/watch?v=rzbFhu6So5U


	
Différentes étapes a suivre:
- Setup Street Fighter sur Pyhton avec gym retro
- Preprocess
- réglages des hyperparamètres (Dans notre cas ça sera fait avec Optuna)
- entrainement du modèle (avec Stable baselines)
- Tester le modèle

Première étape - Setup Street Fighter sur Gym :
-----------------------------------------------
Necessite l'installation de 2 choses
- Open AI Gym
- Gym retro

Qu'est ce que Gym ?:
-------------------
C'est l'outil de base pour créer des modèles de réenforcement. 
lien vers la doc: https://www.gymlibrary.dev/index.html
comme son nom l'indique, c'est la salle de sport ou notre modèle peu s'entrainer et
apprendre a jouer.

[Point historique:
Askip Open AI Gym a pour fondateur la zone 51. ça servait a faire des test militaire
donc cet outils est très serieux, c'est pas du n'importe quoi.]

Et parmis les outils de Gym on a Gym retro qui permet comme son nom l'indique aussi a créer
des modèles sur des jeux retro, en l'occurence ici : Street Fighter.

Partie Technique.
------------------
Evidemment tout sera fait sur Jupyter notebook.
Je conseil de ouf de se créer un environnement dédié pour ce projet parce que il faut une version
de python précise et il faut éviter les problèmes d'incompabatibilité avec vos autres librairies.

### Initialisation
#Ouvir l'anaconda Prompt et taper la commande suivante pour créer un environnement
conda create -n gym_env python=3.7.3 jupyter notebook
#activer votre environnement
conda activate gym_env
#Lancer jupyter notebook depuis ce même terminal pour qu'il sois sur cet environnement
jupyter notebook

### Installation des librairie aux bonnes versions :
# Il suffit d'ouvrir le notebook sur jupyter et de lancer les 3 première cellule de la section
[INITIALISATION DES LIBRAIRIE DE L'ENVIRONNEMENT (extremement important)]
(C'est juste des pip install)

Lien de la ROM du jeu :
-------------
https://wowroms.com/en/roms/sega-genesis-megadrive/street-fighter-ii-special-champion-edition-europe/26496.html

Installer le jeu dans l'émulateur Gym Retro 
--------------------------------------------
- Ouvrir une nouvelle invite de commande anaconda
- activer l'environnement du projet (conda activate gym_env)
- aller grace aux commandes cd jusqu'a votre dossier ou se trouve la/les ROMS que 
t'as installé.
- executer cette commande précise : [python -m retro.import .]
-normalement le message :
Importing StreetFighterIISpecialChampionEdition-Genesis
Imported 1 games
apparaitra.


Si c'est pas le cas sah je sais pas pk. Moi aussi j'ai eu un bug.
J'avais téléchargé la version europe, ça a pas marché
ensuite j'ai installé la version USA
et la ça a marché pour les 2 bizarrement.
anaconda3\envs\gym_env

											Deuxième étape - Preprocessing :
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Objectif de la partie :  crée un environnement Street Fighter simplifié et adapté à l'apprentissage par renforcement. 
L'IA va recevoir des images prétraitées du jeu et apprendre à choisir les actions qui maximisent sa récompense (le score).

Explication des fonctions : 

`__init__(self)`:

C'est le constructeur de la classe `StreetFighter`. Il est appelé automatiquement quand on crée un nouvel objet de cette classe. Son rôle est d'initialiser l'environnement de jeu pour l'IA.

-   `super().__init__()` :  Cette ligne appelle le constructeur de la classe parente (`Env`). Cela permet d'hériter des propriétés et des méthodes de base de la classe `Env` de Gym.
-   `self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)` :  Ici, on définit le type d'observations que l'IA va recevoir.  
    -   `Box` signifie que les observations sont des valeurs numériques continues.
    -   `low=0, high=255` indique que les valeurs seront comprises entre 0 et 255 (ce qui correspond aux valeurs des pixels d'une image en niveaux de gris).
    -   `shape=(84, 84, 1)`  précise la forme des observations : des images de 84 pixels de hauteur, 84 pixels de largeur et 1 canal (niveaux de gris).
    -   `dtype=np.uint8` indique que les valeurs seront stockées sous forme d'entiers non signés de 8 bits.
-   `self.action_space = MultiBinary(12)` :  On définit les actions possibles pour l'IA.  
    -   `MultiBinary(12)` signifie que l'IA peut activer ou désactiver 12 boutons binaires indépendamment (un peu comme appuyer ou relâcher 12 touches sur un clavier). Cela correspondra aux actions du jeu comme "haut", "bas", "gauche", "droite", "poing", "pied", etc.
-   `self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)` :  On crée une instance du jeu Street Fighter II Special Champion Edition.
    -   `retro.make(...)`  utilise la librairie `retro` pour lancer l'émulateur du jeu.
    -   `use_restricted_actions=retro.Actions.FILTERED`  simplifie l'espace d'actions en ne gardant qu'un ensemble d'actions de base.

`reset(self)`:

Cette méthode est appelée au début de chaque nouvelle partie. Elle remet l'environnement à zéro et retourne la première observation.

-   `obs = self.game.reset()` :  On appelle la méthode `reset()` de l'objet `game` pour réinitialiser le jeu. Cela revient à appuyer sur le bouton "Start" pour commencer une nouvelle partie.
-   `obs = self.preprocess(obs)` :  On prétraite l'image brute retournée par le jeu.
-   `self.previous_frame = obs` : On stocke l'image prétraitée dans l'attribut `self.previous_frame`. Cela servira à calculer la différence entre deux images consécutives dans la méthode `step`.
-   `self.score = 0` :  On initialise le score à 0.
-   `return obs` : On retourne l'observation prétraitée à l'agent (l'IA).

`preprocess(self, observation)`:

Cette méthode prend en entrée une image brute du jeu et la prétraite pour la rendre plus facile à analyser par l'IA.

-   `gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)` :  On convertit l'image en niveaux de gris avec la fonction `cvtColor` de la librairie OpenCV (`cv2`).
-   `resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)` : On redimensionne l'image à 84x84 pixels avec la fonction `resize`.  L'argument `interpolation=cv2.INTER_CUBIC`  spécifie le type d'interpolation utilisé pour le redimensionnement.
-   `channels = np.reshape(resize, (84,84,1))` :  On ajoute une dimension à l'image pour la rendre compatible avec les réseaux de neurones convolutifs.  Ces réseaux attendent généralement des images avec 3 dimensions : hauteur, largeur et canaux de couleur.  Même si l'image est en niveaux de gris (1 canal), on ajoute cette dimension pour respecter le format attendu.
-   `return channels` : On retourne l'image prétraitée.

`step(self, action)`:

Cette méthode est la base de l'interaction entre l'IA et l'environnement. Elle prend en entrée une action choisie par l'IA et la transmet au jeu. 
Elle récupère les conséquences de cette action et les retourne à l'IA.

-   `obs, reward, done, info = self.game.step(action)` :  On envoie l'action au jeu avec la méthode `step()` de l'objet `game`. On récupère ensuite :
    -   `obs` : la nouvelle observation (image) après avoir effectué l'action.
    -   `reward` : la récompense obtenue après l'action.
    -   `done` : un booléen qui indique si la partie est terminée (par exemple, si un joueur a perdu toute sa vie).
    -   `info` : un dictionnaire qui contient des informations supplémentaires sur l'état du jeu.
-   `obs = self.preprocess(obs)` :  On prétraite la nouvelle observation.
-   `frame_delta = obs - self.previous_frame` : On calcule la différence entre l'image actuelle et la précédente. Cela permet à l'IA de percevoir le mouvement et les changements dans l'environnement.
-   `self.previous_frame = obs` : On met à jour l'attribut `self.previous_frame` avec l'image actuelle.
-   `reward = info['score'] - self.score` : On calcule la récompense en fonction du changement de score.  On récupère le score actuel dans le dictionnaire `info` et on le compare au score précédent.
-   `self.score = info['score']` : On met à jour l'attribut `self.score` avec le score actuel.
-   `return frame_delta, reward, done, info` : On retourne les informations à l'IA.

`render(self, -args, kwargs)`:

Cette méthode affiche le jeu à l'écran. Elle est utile pour visualiser ce que l'IA "voit" et comment elle interagit avec l'environnement.

`close(self)`:

Cette méthode ferme l'environnement et libère les ressources utilisées par le jeu.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

										Troisième étape - Hyperparameter Tuning :
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Fonction "optimize_ppo":

### But
Ce code définit une fonction nommée `optimize_ppo` qui est conçue pour générer un ensemble de paramètres optimaux pour un algorithme d'apprentissage par renforcement, comme Proximal Policy Optimization (PPO). Ce code est utilisé pour optimiser les hyperparamètres de l'algorithme PPO. Chaque appel à `optimize_ppo(trial)` fournit un nouveau jeu de paramètres à tester, en exploitant les capacités de recherche et de suggestion d'Optuna. Les hyperparamètres comme `n_steps`, `gamma`, `learning_rate`, `clip_range`, et `gae_lambda` sont cruciaux pour équilibrer la stabilité, l'efficacité et la précision du modèle PPO dans les tâches d'apprentissage par renforcement.

Techniquement, cette fonction sert à définir une fonction objectif pour une bibliothèque d'optimisation d'hyperparamètres comme Optuna. Le rôle principal est de tester différentes combinaisons de valeurs d'hyperparamètres et de trouver les valeurs optimales qui permettent au modèle PPO d'obtenir de meilleures performances.

La fonction retourne un dictionnaire contenant 5 hyperparamètres, avec leurs valeurs spécifiées en fonction des suggestions d'Optuna via `trial`. 
1. `n_steps` :
   - Représente le nombre de pas (ou transitions) collectés avant chaque mise à jour du modèle.
   - Optuna suggère ici une valeur entière entre 2048 et 8192 (inclus).
   - Plus cette valeur est grande, plus l'entraînement sera stable, mais cela nécessitera plus de mémoire.

2. `gamma` :
   - C'est le facteur de discount qui détermine dans quelle mesure les récompenses futures sont prises en compte.
   - Optuna utilise une distribution logarithmique uniforme pour choisir une valeur entre 0.8 et 0.9999.
   - Une valeur proche de 1 favorise une planification à long terme.

3. `learning_rate` :
   - Le taux d'apprentissage contrôle la vitesse à laquelle le modèle ajuste ses poids.
   - Optuna propose une valeur dans une plage logarithmique entre 1e-5 (0.00001) et 1e-4 (0.0001).
   - Cela garantit que des valeurs très petites sont également testées, souvent nécessaires pour stabiliser l'apprentissage.

4. `clip_range` :
   - Utilisé dans PPO pour limiter l'ampleur des mises à jour des politiques (Policy Gradient Clipping).
   - Optuna choisit une valeur uniforme entre 0.1 et 0.4.
   - Cette valeur aide à contrôler les grands changements, ce qui peut améliorer la stabilité.

5. `gae_lambda` :
   - Le facteur lambda de Generalized Advantage Estimation (GAE), qui aide à réduire la variance des estimations de l'avantage.
   - Optuna propose une valeur uniforme entre 0.8 et 0.99.
   - Une valeur élevée favorise une meilleure approximation de l'avantage, tandis qu'une valeur plus basse peut favoriser une variance plus faible.

### 3. Fonctionnement avec Optuna
- `trial.suggest_*` : Ces méthodes permettent de définir des plages de recherche pour chaque hyperparamètre :
  - `suggest_int` : Pour des valeurs entières.
  - `suggest_loguniform` : Pour des valeurs continues sur une échelle logarithmique.
  - `suggest_uniform` : Pour des valeurs continues sur une échelle linéaire.
  - Pendant l'optimisation, Optuna explore ces plages pour trouver la meilleure combinaison.

---

Fonction "optimize_agent":

### But
La fonction **`optimize_agent(trial)`** entraîne un agent d'apprentissage par renforcement basé sur l'algorithme **PPO** (Proximal Policy Optimization). Elle évalue ensuite les performances de cet agent en renvoyant une récompense moyenne obtenue sur plusieurs épisodes. L'objectif est de trouver les meilleurs hyperparamètres pour maximiser cette récompense.

---

### **1. Optimisation des hyperparamètres**
```python
model_params = optimize_ppo(trial)
```
- La fonction **`optimize_ppo(trial)`** (expliquée précédemment) génère un ensemble de **paramètres optimaux** pour PPO.
- Ces paramètres seront utilisés pour configurer le modèle.

---

### **2. Création de l'environnement**
- **`StreetFighter()`** : Crée l'environnement du jeu vidéo (ex. : "Street Fighter"). C'est là où l'agent interagit.
- **`Monitor(env, LOG_DIR)`** : Ajoute un système de suivi pour enregistrer les métriques, comme les récompenses par épisode, dans un répertoire de logs défini par `LOG_DIR`.
- **`DummyVecEnv([lambda: env])`** : Transforme l'environnement en un environnement vectorisé, permettant de le gérer plus efficacement dans des algorithmes comme PPO.
- **`VecFrameStack(env, 4, channels_order='last')`** : Empile 4 images consécutives (frames) de l'environnement pour capturer des informations temporelles. Cela est particulièrement utile pour les jeux vidéo où le mouvement est important.

### **3. Création et entraînement du modèle**

- **`PPO('CnnPolicy', ...)`** : Crée un modèle PPO avec une architecture de réseau neuronal basée sur des **CNN (Convolutional Neural Networks)**, adaptée aux environnements avec des entrées visuelles comme des images.
- **`tensorboard_log=LOG_DIR`** : Permet de visualiser les métriques d'entraînement dans TensorBoard.
- **`verbose=0`** : Désactive les messages d'entraînement dans la console.
- **`model.learn(total_timesteps=30000)`** : Entraîne le modèle PPO pendant **30 000 étapes de simulation**.

### **4. Évaluation des performances**
- **`evaluate_policy(model, env, n_eval_episodes=5)`** : Évalue le modèle en le testant sur **5 épisodes** dans l'environnement.
- **`mean_reward`** : La récompense moyenne obtenue sur ces 5 épisodes, qui reflète les performances de l'agent.

### **5. Sauvegarde du modèle**
- Le modèle entraîné est sauvegardé dans un répertoire défini par **`OPT_DIR`** avec un nom unique basé sur le numéro d'essai (**`trial.number`**).
- Cela permet de conserver les modèles entraînés pour des hyperparamètres spécifiques.

### **6. Gestion des erreurs**
- Si une erreur se produit lors de l'exécution (ex. : problème de mémoire, environnement non configuré), la fonction retourne une **valeur de pénalité** de **-1000**.
- Cela indique que les hyperparamètres testés dans ce cas ne fonctionnent pas correctement.

### **Résumé**
1. La fonction configure et entraîne un agent PPO sur l'environnement de jeu "Street Fighter".
2. Elle évalue les performances de l'agent en calculant une récompense moyenne sur 5 épisodes.
3. Si tout se passe bien, le modèle est sauvegardé et la récompense moyenne est retournée.
4. En cas de problème, une valeur de pénalité est retournée pour signaler une tentative d'optimisation échouée.








